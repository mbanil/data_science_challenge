{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling as pp\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Anil:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Zeiss_task</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f4a62c4c88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Zeiss_task\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/insurance_claims.csv', header=True, sep=',', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+------------------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "|months_as_customer|age|policy_number|policy_bind_date|policy_state|policy_csl|policy_deductable|policy_annual_premium|umbrella_limit|insured_zip|insured_sex|insured_education_level|insured_occupation|insured_hobbies|insured_relationship|capital-gains|capital-loss|incident_date|incident_type           |collision_type|incident_severity|authorities_contacted|incident_state|incident_city|incident_location|incident_hour_of_the_day|number_of_vehicles_involved|property_damage|bodily_injuries|witnesses|police_report_available|total_claim_amount|injury_claim|property_claim|vehicle_claim|auto_make|auto_model|auto_year|fraud_reported|\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+------------------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "|328               |48 |521585       |2014-10-17      |OH          |250/500   |1000             |1406.91              |0             |466132     |MALE       |MD                     |craft-repair      |sleeping       |husband             |53300        |0           |2015-01-25   |Single Vehicle Collision|Side Collision|Major Damage     |Police               |SC            |Columbus     |9935 4th Drive   |5                       |1                          |YES            |1              |2        |YES                    |71610             |6510        |13020         |52080        |Saab     |92x       |2004     |Y             |\n",
      "|228               |42 |342868       |2006-06-27      |IN          |250/500   |2000             |1197.22              |5000000       |468176     |MALE       |MD                     |machine-op-inspct |reading        |other-relative      |0            |0           |2015-01-21   |Vehicle Theft           |?             |Minor Damage     |Police               |VA            |Riverwood    |6608 MLK Hwy     |8                       |1                          |?              |0              |0        |?                      |5070              |780         |780           |3510         |Mercedes |E400      |2007     |Y             |\n",
      "+------------------+---+-------------+----------------+------------+----------+-----------------+---------------------+--------------+-----------+-----------+-----------------------+------------------+---------------+--------------------+-------------+------------+-------------+------------------------+--------------+-----------------+---------------------+--------------+-------------+-----------------+------------------------+---------------------------+---------------+---------------+---------+-----------------------+------------------+------------+--------------+-------------+---------+----------+---------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- months_as_customer: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- policy_csl: string (nullable = true)\n",
      " |-- policy_deductable: integer (nullable = true)\n",
      " |-- policy_annual_premium: double (nullable = true)\n",
      " |-- umbrella_limit: integer (nullable = true)\n",
      " |-- insured_sex: string (nullable = true)\n",
      " |-- insured_education_level: string (nullable = true)\n",
      " |-- insured_hobbies: string (nullable = true)\n",
      " |-- insured_relationship: string (nullable = true)\n",
      " |-- capital-gains: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- collision_type: string (nullable = true)\n",
      " |-- incident_severity: string (nullable = true)\n",
      " |-- authorities_contacted: string (nullable = true)\n",
      " |-- incident_state: string (nullable = true)\n",
      " |-- incident_city: string (nullable = true)\n",
      " |-- incident_hour_of_the_day: integer (nullable = true)\n",
      " |-- number_of_vehicles_involved: integer (nullable = true)\n",
      " |-- property_damage: string (nullable = true)\n",
      " |-- bodily_injuries: integer (nullable = true)\n",
      " |-- witnesses: integer (nullable = true)\n",
      " |-- police_report_available: string (nullable = true)\n",
      " |-- total_claim_amount: integer (nullable = true)\n",
      " |-- injury_claim: integer (nullable = true)\n",
      " |-- property_claim: integer (nullable = true)\n",
      " |-- vehicle_claim: integer (nullable = true)\n",
      " |-- auto_year: integer (nullable = true)\n",
      " |-- fraud_reported: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ('policy_number','policy_bind_date','policy_state','insured_zip','incident_location','incident_date','auto_make','auto_model','insured_occupation')\n",
    "\n",
    "df = df.drop(*cols)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- months_as_customer: integer (nullable = true)\n",
      " |-- policy_csl: string (nullable = true)\n",
      " |-- policy_deductable: integer (nullable = true)\n",
      " |-- policy_annual_premium: double (nullable = true)\n",
      " |-- umbrella_limit: integer (nullable = true)\n",
      " |-- insured_sex: string (nullable = true)\n",
      " |-- insured_education_level: string (nullable = true)\n",
      " |-- insured_hobbies: string (nullable = true)\n",
      " |-- insured_relationship: string (nullable = true)\n",
      " |-- capital-gains: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- incident_type: string (nullable = true)\n",
      " |-- collision_type: string (nullable = true)\n",
      " |-- incident_severity: string (nullable = true)\n",
      " |-- authorities_contacted: string (nullable = true)\n",
      " |-- incident_state: string (nullable = true)\n",
      " |-- incident_city: string (nullable = true)\n",
      " |-- incident_hour_of_the_day: integer (nullable = true)\n",
      " |-- number_of_vehicles_involved: integer (nullable = true)\n",
      " |-- property_damage: string (nullable = true)\n",
      " |-- bodily_injuries: integer (nullable = true)\n",
      " |-- witnesses: integer (nullable = true)\n",
      " |-- police_report_available: string (nullable = true)\n",
      " |-- injury_claim: integer (nullable = true)\n",
      " |-- property_claim: integer (nullable = true)\n",
      " |-- vehicle_claim: integer (nullable = true)\n",
      " |-- auto_year: integer (nullable = true)\n",
      " |-- fraud_reported: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ('age', 'total_claim_amount')\n",
    "\n",
    "df = df.drop(*cols)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "drop() got an unexpected keyword argument 'inplace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5892\\3489457708.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mto_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'policy_number'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'policy_bind_date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'policy_state'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'insured_zip'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'incident_location'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'incident_date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'auto_make'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'auto_model'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'insured_occupation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: drop() got an unexpected keyword argument 'inplace'"
     ]
    }
   ],
   "source": [
    "to_drop = ['policy_number','policy_bind_date','policy_state','insured_zip','incident_location','incident_date','auto_make','auto_model','insured_occupation']\n",
    "df.drop(to_drop, inplace = True, axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5892\\3104464723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'insured_hobbies'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'insured_hobbies'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;34m'Other'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'chess'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'cross-fit'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df['insured_hobbies']=df['insured_hobbies'].apply(lambda x :'Other' if x!='chess' and x!='cross-fit' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "df = df.withColumn('insured_hobbies', when(df['insured_hobbies'] =='chess',df['insured_hobbies'])\\\n",
    "    .when(df['insured_hobbies'] =='cross-fit',df['insured_hobbies'])\\\n",
    "    .otherwise('Others'))\n",
    "\n",
    "\n",
    "# when(df.address.endswith('Rd'),regexp_replace(df.address,'Rd','Road')) \\\n",
    "#    .when(df.address.endswith('St'),regexp_replace(df.address,'St','Street')) \\\n",
    "#    .when(df.address.endswith('Ave'),regexp_replace(df.address,'Ave','Avenue')) \\\n",
    "#    .otherwise(df.address)) \\\n",
    "#    .show(truncate=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('fraud_reported', when(df['fraud_reported'] =='Y',1)\\\n",
    "    .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.where(\"umbrella_limit>=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({'umbrella_limit': 'min'}).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode =  df.groupby(\"collision_type\").count().orderBy(\"count\", ascending=False).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('collision_type', when(df['collision_type'] =='?',mode)\\\n",
    "    .otherwise(df['collision_type']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('property_damage', when(df['property_damage'] =='?','NO')\\\n",
    "    .otherwise(df['property_damage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('police_report_available', when(df['police_report_available'] =='?','NO')\\\n",
    "    .otherwise(df['police_report_available']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['collision_type'].fillna(df['collision_type'].mode()[0], inplace = True)\n",
    "\n",
    "df['property_damage'].fillna('NO', inplace = True)\n",
    "\n",
    "df['police_report_available'].fillna('NO', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(police_report_available='YES'), Row(police_report_available='NO')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_df.select(\"insured_hobbies\").sample(False, 0.1, seed=9).limit(20).show()\n",
    "df.select('police_report_available').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Output column insured_hobbiesIndex already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5892\\1932134383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minsured_hobbies_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"insured_hobbies\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"insured_hobbiesIndex\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minsured_hobbies_indexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0monehotencoder_age_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"insured_hobbiesIndex\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"insured_hobbies_vec\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monehotencoder_age_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zeiss\\lib\\site-packages\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zeiss\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zeiss\\lib\\site-packages\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zeiss\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1322\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\zeiss\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Output column insured_hobbiesIndex already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "insured_hobbies_indexer = StringIndexer(inputCol=\"insured_hobbies\", outputCol=\"insured_hobbiesIndex\")\n",
    "df = insured_hobbies_indexer.fit(df).transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder_age_vector = OneHotEncoder(inputCol=\"insured_hobbiesIndex\", outputCol=\"insured_hobbies_vec\")\n",
    "df = onehotencoder_age_vector.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "name = 'insured_hobbies'\n",
    "udf = UserDefinedFunction(lambda x :'Other' if x!='chess' or x!='cross-fit' else x)\n",
    "new_df = df.withColumn('insured_hobbies', udf(df.insured_hobbies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(insured_hobbies_vec=SparseVector(2, {0: 1.0})),\n",
       " Row(insured_hobbies_vec=SparseVector(2, {1: 1.0})),\n",
       " Row(insured_hobbies_vec=SparseVector(2, {}))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_df.select(\"insured_hobbies\").sample(False, 0.1, seed=9).limit(20).show()\n",
    "df.select('insured_hobbies_vec').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(months_as_customer,StringType,true),\n",
       " StructField(policy_csl,StringType,true),\n",
       " StructField(policy_deductable,StringType,true),\n",
       " StructField(policy_annual_premium,StringType,true),\n",
       " StructField(umbrella_limit,StringType,true),\n",
       " StructField(insured_sex,StringType,true),\n",
       " StructField(insured_education_level,StringType,true),\n",
       " StructField(insured_hobbies,StringType,true),\n",
       " StructField(insured_relationship,StringType,true),\n",
       " StructField(capital-gains,StringType,true),\n",
       " StructField(capital-loss,StringType,true),\n",
       " StructField(incident_type,StringType,true),\n",
       " StructField(collision_type,StringType,true),\n",
       " StructField(incident_severity,StringType,true),\n",
       " StructField(authorities_contacted,StringType,true),\n",
       " StructField(incident_state,StringType,true),\n",
       " StructField(incident_city,StringType,true),\n",
       " StructField(incident_hour_of_the_day,StringType,true),\n",
       " StructField(number_of_vehicles_involved,StringType,true),\n",
       " StructField(property_damage,StringType,true),\n",
       " StructField(bodily_injuries,StringType,true),\n",
       " StructField(witnesses,StringType,true),\n",
       " StructField(police_report_available,StringType,true),\n",
       " StructField(injury_claim,StringType,true),\n",
       " StructField(property_claim,StringType,true),\n",
       " StructField(vehicle_claim,StringType,true),\n",
       " StructField(auto_year,StringType,true),\n",
       " StructField(fraud_reported,StringType,true)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def onehot(df,col):\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col+'Index')\n",
    "\n",
    "    df = indexer.fit(df).transform(df)\n",
    "\n",
    "    onehotencoder = OneHotEncoder(inputCol=col+'Index', outputCol=col+'Vector')\n",
    "\n",
    "    df = onehotencoder.fit(df).transform(df)\n",
    "\n",
    "    cols_drop = (col,col+'Index')\n",
    "    df = df.drop(*cols_drop)\n",
    "\n",
    "    return df\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = onehot(df,'incident_severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(incident_severityVector=SparseVector(3, {2: 1.0})),\n",
       " Row(incident_severityVector=SparseVector(3, {1: 1.0})),\n",
       " Row(incident_severityVector=SparseVector(3, {0: 1.0})),\n",
       " Row(incident_severityVector=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('incident_severityVector').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(findspark.init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import schedule\n",
    "import http.client\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/insurance_claims.csv', header=True, sep=',', inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_pandas.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing json \n",
    "json_object = json.dumps(df_json, indent = 4)\n",
    "  \n",
    "# Writing to sample.json\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = http.client.HTTPConnection('127.0.0.1', 8000)\n",
    "data = pd.read_csv('Data_repository/Data/test.csv')\n",
    "data = data.to_json()\n",
    "c.request('POST', '/get_prediction_results', json.dumps(data))\n",
    "result = c.getresponse().read()\n",
    "prediction_result = json.loads(result)\n",
    "prediction_result_df = pd.read_json(prediction_result)\n",
    "print(prediction_result_df.head())\n",
    "return prediction_result_df\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    schedule.every(0).minutes.do(get_prediction_result)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '{\"months_as_customer\":{\"0\":328},\"age\":{\"0\":48},\"policy_number\":{\"0\":521585},\"policy_bind_date\":{\"0\":1413504000000},\"policy_state\":{\"0\":\"OH\"},\"policy_csl\":{\"0\":\"250\\\\/500\"},\"policy_deductable\":{\"0\":1000},\"policy_annual_premium\":{\"0\":1406.91},\"umbrella_limit\":{\"0\":0},\"insured_zip\":{\"0\":466132},\"insured_sex\":{\"0\":\"MALE\"},\"insured_education_level\":{\"0\":\"MD\"},\"insured_occupation\":{\"0\":\"craft-repair\"},\"insured_hobbies\":{\"0\":\"sleeping\"},\"insured_relationship\":{\"0\":\"husband\"},\"capital-gains\":{\"0\":53300},\"capital-loss\":{\"0\":0},\"incident_date\":{\"0\":1422144000000},\"incident_type\":{\"0\":\"Single Vehicle Collision\"},\"collision_type\":{\"0\":\"Side Collision\"},\"incident_severity\":{\"0\":\"Major Damage\"},\"authorities_contacted\":{\"0\":\"Police\"},\"incident_state\":{\"0\":\"SC\"},\"incident_city\":{\"0\":\"Columbus\"},\"incident_location\":{\"0\":\"9935 4th Drive\"},\"incident_hour_of_the_day\":{\"0\":5},\"number_of_vehicles_involved\":{\"0\":1},\"property_damage\":{\"0\":\"YES\"},\"bodily_injuries\":{\"0\":1},\"witnesses\":{\"0\":2},\"police_report_available\":{\"0\":\"YES\"},\"total_claim_amount\":{\"0\":71610},\"injury_claim\":{\"0\":6510},\"property_claim\":{\"0\":13020},\"vehicle_claim\":{\"0\":52080},\"auto_make\":{\"0\":\"Saab\"},\"auto_model\":{\"0\":\"92x\"},\"auto_year\":{\"0\":2004},\"fraud_reported\":{\"0\":\"Y\"}}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_json = json.loads(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.DataFrame.from_dict(a_json)\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "def get_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Zeiss Task')\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--app_name\", type=str, default=\"Zeiss_classification_task\")\n",
    "    parser.add_argument(\"--data_filename\", type=str, default=\"./data/insurance_claims.csv\",\n",
    "                                    help=\"data file in csv format\")\n",
    "    parser.add_argument(\"--columns_to_drop\", type=list, \n",
    "                        default=['policy_number','policy_bind_date','policy_state','insured_zip','incident_location',\n",
    "                        'incident_date','auto_make','auto_model','insured_occupation','age', 'total_claim_amount'])\n",
    "    parser.add_argument(\"--store_schema\", type=bool, default=False)\n",
    "    parser.add_argument(\"--schema_path\", type=str, default=\"/src/data/feature_store/\")\n",
    "    parser.add_argument(\"--columns_to_encode\", type=list, \n",
    "                        default=['policy_csl', 'insured_sex', 'insured_education_level','insured_hobbies', 'insured_relationship',\n",
    "                        'incident_type', 'incident_severity','authorities_contacted', 'incident_state', 'incident_city','collision_type'])\n",
    "    parser.add_argument(\"--preprocess_hobbies\", type=bool, default=True)\n",
    "    parser.add_argument(\"--tune_hyper_params\", type=bool, default=False)\n",
    "    parser.add_argument(\"--best_hyper_params_filepath\", type=str, default='/src/data/best_hyper_params')\n",
    "    parser.add_argument(\"--model_path\", type=str, default='/src/model')\n",
    "    parser.add_argument(\"--hyper_params\", type=dict, \n",
    "                        default={'criterion': 'gini', \n",
    "                                    'max_depth': 5, \n",
    "                                    'min_samples_leaf': 2, \n",
    "                                    'min_samples_split': 2\n",
    "                                    })\n",
    "    parser.add_argument(\"--target\", type=str, default='fraud_reported')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = sys.argv[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2308\\1288120579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_spark\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\My Documents\\zeiss_task\\src\\utils\\utils.py\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[1;34m(df, args)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns_to_encode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\My Documents\\zeiss_task\\src\\utils\\utils.py\u001b[0m in \u001b[0;36mencode_data\u001b[1;34m(df, col, storeSchema, schema_path)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mschema_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/schemaData/StringIndexer/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[0monehotencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mschema_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/schemaData/OneHotEncoder/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monehotencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "from src.utils.utils import  data_preprocessing\n",
    "\n",
    "df_processed = data_preprocessing(df_spark, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer.load(os.getcwd() + args.schema_path + '/schemaData/' + 'incident_city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.StringIndexer"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "411a9559551a45b396565ba67c60c5824103901d9a149aa4a1803e9672102111"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('zeiss')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
